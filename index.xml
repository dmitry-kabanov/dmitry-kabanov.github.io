<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Website of Dmitry Kabanov</title>
    <link>https://dmitrykabanov.com/</link>
    <description>Recent content on Website of Dmitry Kabanov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Jun 2021 10:49:00 +0200</lastBuildDate><atom:link href="https://dmitrykabanov.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Need for software testing</title>
      <link>https://dmitrykabanov.com/blog/2021/need-for-software-testing/</link>
      <pubDate>Mon, 28 Jun 2021 10:49:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2021/need-for-software-testing/</guid>
      <description>Recently I was attending the class on uncertainty quantification for partial differential equations. As a postdoc, I do participate in the class only partially, however, I have a group of students that I am helping with the homework.
The first homework required use of Monte Carlo methods with differential equations with random coefficients to estimate uncertainty in the solutions of these equations. As the person with experience in writing PDE solvers, I wrote the solver for the PDE that we used in this homework and shared it with others.</description>
    </item>
    
    <item>
      <title>Books on writing science</title>
      <link>https://dmitrykabanov.com/blog/2021/books-on-writing-science/</link>
      <pubDate>Sat, 15 May 2021 12:16:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2021/books-on-writing-science/</guid>
      <description>To be successful in academia, it is very important to have a good skill in writing and to write often and productively. Because the good skill in writing is rarely taught, one should learn it from reading books on the topic.
Recommended books on this topic are:
  Belcher W. Writing Your Journal Article in Twelve Weeks, Second Edition
I haven&amp;rsquo;t read it but it has very high grades in reviews.</description>
    </item>
    
    <item>
      <title>Basic terms in the probability theory</title>
      <link>https://dmitrykabanov.com/blog/2021/basic-terms-probability-theory/</link>
      <pubDate>Sat, 24 Apr 2021 18:54:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2021/basic-terms-probability-theory/</guid>
      <description>&lt;p&gt;Basic terms in probability theory are outcomes, sample space, events, and
probability measure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Outcomes and sample space&lt;/strong&gt;
.
When you conduct a random experiment, there is a set of possible results of this
experiment.
This results are called &lt;em&gt;outcomes&lt;/em&gt;.
All possible outcomes together form a set, which is called &lt;em&gt;sample space&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Colaborate in Latex: Overleaf and local git</title>
      <link>https://dmitrykabanov.com/blog/2021/collaborate-overleaf-git/</link>
      <pubDate>Fri, 19 Feb 2021 21:55:00 +0100</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2021/collaborate-overleaf-git/</guid>
      <description>When you write a paper in \(\LaTeX\) and your collaborators prefer to use Overleaf but you prefer to use local setup, then you can do it with the help of the git version control system.
Overall, the process is the following. You create a local git repository and sync it to a remote git service of your choice. Then you create a project in Overleaf. In the project settings you can find the link which you add to your git setup as the second remote server.</description>
    </item>
    
    <item>
      <title>How to handle python job cancelation in Slurm job manager</title>
      <link>https://dmitrykabanov.com/blog/2020/04-job-cancelation-in-slurm/</link>
      <pubDate>Sat, 19 Dec 2020 23:01:00 +0100</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2020/04-job-cancelation-in-slurm/</guid>
      <description>If you use Slurm job manager to run jobs on shared cluster, it often occurs that your job reaches the time limit and is terminated by Slurm. To allow a user to deal with the job termination, Slurm does this in two stages: first, the job receives SIGTERM signal that indicates that the job will be killed soon, and then the job receives SIGKILL signal that actually kills it. The time interval between these two signals is specified via Slurm&amp;rsquo;s configuration parameter KillWait.</description>
    </item>
    
    <item>
      <title>Saving state for tf.function-decorated functions</title>
      <link>https://dmitrykabanov.com/blog/2020/03-saving-state-tf-function/</link>
      <pubDate>Fri, 31 Jul 2020 12:19:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2020/03-saving-state-tf-function/</guid>
      <description>When you decorate a function with `tf.function` decorator, sometimes you need to keep state between invocations of this function.
The problem here is that the changes to the state will not be visible in the decorated function if the state is saved in the Python variables.
To illustrate the problem, Tensorflow 2.2 is used:
import tensorflow as tf print(tf.__version__) 2.2.0 To see the problem, let&amp;rsquo;s consider the following code. Let&amp;rsquo;s assume that we need to scale a given Tensor `x` and we do it using `tf.</description>
    </item>
    
    <item>
      <title>Using `tf.function` for performance in Tensorflow 2</title>
      <link>https://dmitrykabanov.com/blog/2020/02-tf-function-performance/</link>
      <pubDate>Tue, 28 Jul 2020 13:54:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2020/02-tf-function-performance/</guid>
      <description>Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient.</description>
    </item>
    
    <item>
      <title>Hyperopt Basics</title>
      <link>https://dmitrykabanov.com/blog/2020/01-hyperopt-basics/</link>
      <pubDate>Thu, 12 Mar 2020 15:16:00 +0100</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2020/01-hyperopt-basics/</guid>
      <description>This is an introduction to using Hyperopt library. I will use mostly terminology of machine learning (ML) as this library appeared in the ML community.
Hyperopt library is used to choose the hyperparameters, that is, parameters that must be set before the learning process. The learning process is the process of fitting a given model to some dataset, which is done by minimization of some function.
For example, when you fit model \(\hat f (x)\) by optimizing function \[ \frac{1}{N} \sum_{i=1}^N \left( y_i - \hat f (x_i) \right)^2 + \lambda R(f), \]</description>
    </item>
    
    <item>
      <title>Borel σ-algebra</title>
      <link>https://dmitrykabanov.com/blog/2019/borel-sigma-algebra/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2019/borel-sigma-algebra/</guid>
      <description>When one studies probability theory, a notion of a Borel $σ$-algebra arises. Here we will look at what this is.
First thing is to understand what $σ$-algebra means.
Let \(X\) be a set. Then if we have another set \(\mathcal B\), which we call σ-algebra over \(X\), if it has the following properties:
  it consists of subsets of \(X\),
  \(X \in \mathcal B\),
  for any \(b \in \mathcal B\), it follows that \(b^\C \in \mathcal B\), where superscript \(\C\) denotes complement,</description>
    </item>
    
  </channel>
</rss>
