<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Website of Dmitry Kabanov</title>
    <link>https://dmitrykabanov.com/blog/</link>
    <description>Recent content in Blogs on Website of Dmitry Kabanov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jul 2020 13:54:00 +0200</lastBuildDate>
    
	<atom:link href="https://dmitrykabanov.com/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using `tf.function` for performance in Tensorflow 2</title>
      <link>https://dmitrykabanov.com/blog/2020/02-tf-function-performance/</link>
      <pubDate>Tue, 28 Jul 2020 13:54:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2020/02-tf-function-performance/</guid>
      <description>Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient.</description>
    </item>
    
    <item>
      <title>Hyperopt Basics</title>
      <link>https://dmitrykabanov.com/blog/2020/01-hyperopt-basics/</link>
      <pubDate>Thu, 12 Mar 2020 15:16:00 +0100</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2020/01-hyperopt-basics/</guid>
      <description>This is an introduction to using Hyperopt library. I will use mostly terminology of machine learning (ML) as this library appeared in the ML community.
Hyperopt library is used to choose the hyperparameters, that is, parameters that must be set before the learning process. The learning process is the process of fitting a given model to some dataset, which is done by minimization of some function.
For example, when you fit model \(\hat f (x)\) by optimizing function \[ \frac{1}{N} \sum_{i=1}^N \left( y_i - \hat f (x_i) \right)^2 + \lambda R(f), \]</description>
    </item>
    
    <item>
      <title>Borel σ-algebra</title>
      <link>https://dmitrykabanov.com/blog/2019/borel-sigma-algebra/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0200</pubDate>
      
      <guid>https://dmitrykabanov.com/blog/2019/borel-sigma-algebra/</guid>
      <description>When one studies probability theory, a notion of a Borel $σ$-algebra arises. Here we will look at what this is.
First thing is to understand what $σ$-algebra means.
Let \(X\) be a set. Then if we have another set \(\mathcal B\), which we call σ-algebra over \(X\), if it has the following properties:
  it consists of subsets of \(X\),
  \(X \in \mathcal B\),
  for any \(b \in \mathcal B\), it follows that \(b^\C \in \mathcal B\), where superscript \(\C\) denotes complement,</description>
    </item>
    
  </channel>
</rss>