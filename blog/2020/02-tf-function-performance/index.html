<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Using `tf.function` for performance in Tensorflow 2 | Website of Dmitry Kabanov</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient.">
    <meta name="generator" content="Hugo 0.114.0">
    
    
    
    
      <meta name="robots" content="index, follow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.b063366857c44bfaa0bc6b9f781d1ebdaaf347b1c38be070adb5d5ebca508341.css" >



  
    <link rel="stylesheet" href="/css/chroma.css">
  

    


<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
 MathJax.Hub.Config({
     tex2jax: {
         inlineMath: [['$','$'], ['\\(','\\)']],
         displayMath: [['$$','$$'], ['\\[', '\\]']],
         processEscapes: true,
         processEnvironments: true,
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
         TeX: { equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"] }
     }
 });
 MathJax.Hub.Queue(function() {
     
     
     
     var all = MathJax.Hub.getAllJax(), i;
     for(i = 0; i < all.length; i += 1) {
         all[i].SourceElement().parentNode.className += ' has-jax';
     }
 });

 MathJax.Hub.Config({
     
     TeX: {
         equationNumbers: { autoNumber: "AMS" },
         Macros: {
             C: "{\\mathrm{C}}",
             E: "{\\mathbb{E}}",
             P: "{\\mathbb{P}}",
             R: "{\\mathbb{R}}",
             normal: "{\\mathrm{N}}",
             uniform: "{\\mathrm{Unif}}",
         },
     },
 });
</script>

    
    
      

    

    
    
    <meta property="og:title" content="Using `tf.function` for performance in Tensorflow 2" />
<meta property="og:description" content="Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dmitrykabanov.com/blog/2020/02-tf-function-performance/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2020-07-28T13:54:00+02:00" />
<meta property="article:modified_time" content="2020-07-28T13:54:00+02:00" />
<meta itemprop="name" content="Using `tf.function` for performance in Tensorflow 2">
<meta itemprop="description" content="Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient."><meta itemprop="datePublished" content="2020-07-28T13:54:00+02:00" />
<meta itemprop="dateModified" content="2020-07-28T13:54:00+02:00" />
<meta itemprop="wordCount" content="299">
<meta itemprop="keywords" content="tensorflow," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using `tf.function` for performance in Tensorflow 2"/>
<meta name="twitter:description" content="Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient."/>

      
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RZQVXP6B2M"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-RZQVXP6B2M', { 'anonymize_ip': false });
}
</script>

    
	
  </head>

  <body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Website of Dmitry Kabanov
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About me page">
              About me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/blog/" title="Blogs page">
              Blogs
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        BLOGS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Using `tf.function` for performance in Tensorflow 2</h1>
      
      <p class="tracked">
        By <strong>Dmitry Kabanov</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-07-28T13:54:00+02:00">July 28, 2020</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Tensorflow 2 uses so called Eager mode by default.
In this mode, it is easy to define tensors interactively, for example, in
ipython and see their values.
However, in Eager mode the execution is slow, which becomes noticable during
model training.</p>
<p>Tensorflow 2 offers another mode of execution called Graph mode.
In this mode, first the computational graph is created and then used to compute
loss function and its gradient.
This mode is more performance efficient.</p>
<p>It is possible to convert a function automatically to the Graph mode by
decorating it with the <a href="https://www.tensorflow.org/guide/function">tf.function decorator</a>.
This gives a simple way to speed up a function.</p>
<p>Let&rsquo;s compare the execution time when using each of these modes.
I will use Tensorflow 2.2 in this post:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tensorflow version: &#34;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Tensorflow version:  2.2.0
</span></span></code></pre></div><p>As an example, consider computation of the quantity
\[
\left(\sin x \right)^2 + \left( \cos y \right)^2
\]
with \(x\) and \(y\) being two-dimensional arrays (matrices).
We define two functions (one without and another with the decorator) and
create the arrays:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">f_slow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tf.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">f_fast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">4000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">4000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</span></span></code></pre></div><p>Now let&rsquo;s measure the difference in the performance:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">timeit</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">t_slow</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;f_slow(x, y)&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import f_slow, x, y&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">number</span><span class="o">=</span><span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t_slow</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">t_fast</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;f_fast(x, y)&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import f_fast, x, y&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">number</span><span class="o">=</span><span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t_fast</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">1.02903594000054
</span></span><span class="line"><span class="cl">0.6089697299994441
</span></span></code></pre></div><p>You can see that the runtime of the decorated function is only 60% of that
of the pure Python function.
In general, <code>tf.function</code> decorator should be used whenever you have some
computation-heavy function.
Good example is one training step for a neural network in which you need to
evaluate loss function, compute its gradient and then update the neural-network
parameters.</p>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/tensorflow/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">tensorflow</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://dmitrykabanov.com/" >
    &copy;  Website of Dmitry Kabanov 2023 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
