<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Website of Dmitry Kabanov  | Using `tf.function` for performance in Tensorflow 2</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.80.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    
      <link rel="stylesheet" href="/css/chroma.css">
    

    
      
    

    

    <meta property="og:title" content="Using `tf.function` for performance in Tensorflow 2" />
<meta property="og:description" content="Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dmitrykabanov.com/blog/2020/02-tf-function-performance/" />
<meta property="article:published_time" content="2020-07-28T13:54:00+02:00" />
<meta property="article:modified_time" content="2020-07-28T13:54:00+02:00" />
<meta itemprop="name" content="Using `tf.function` for performance in Tensorflow 2">
<meta itemprop="description" content="Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient.">
<meta itemprop="datePublished" content="2020-07-28T13:54:00+02:00" />
<meta itemprop="dateModified" content="2020-07-28T13:54:00+02:00" />
<meta itemprop="wordCount" content="299">



<meta itemprop="keywords" content="tensorflow," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using `tf.function` for performance in Tensorflow 2"/>
<meta name="twitter:description" content="Tensorflow 2 uses so called Eager mode by default. In this mode, it is easy to define tensors interactively, for example, in ipython and see their values. However, in Eager mode the execution is slow, which becomes noticable during model training.
Tensorflow 2 offers another mode of execution called Graph mode. In this mode, first the computational graph is created and then used to compute loss function and its gradient. This mode is more performance efficient."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://dmitrykabanov.com" class="f3 fw2 hover-white no-underline white-90 dib">
      Website of Dmitry Kabanov
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About me page">
              About me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/blog/" title="Blogs page">
              Blogs
            </a>
          </li>
          
        </ul>
      
      











    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        BLOGS
      </p>
      <h1 class="f1 athelas mb1">Using `tf.function` for performance in Tensorflow 2</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2020-07-28T13:54:00&#43;02:00">July 28, 2020</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>Tensorflow 2 uses so called Eager mode by default.
In this mode, it is easy to define tensors interactively, for example, in
ipython and see their values.
However, in Eager mode the execution is slow, which becomes noticable during
model training.</p>
<p>Tensorflow 2 offers another mode of execution called Graph mode.
In this mode, first the computational graph is created and then used to compute
loss function and its gradient.
This mode is more performance efficient.</p>
<p>It is possible to convert a function automatically to the Graph mode by
decorating it with the <a href="https://www.tensorflow.org/guide/function">tf.function decorator</a>.
This gives a simple way to speed up a function.</p>
<p>Let&rsquo;s compare the execution time when using each of these modes.
I will use Tensorflow 2.2 in this post:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Tensorflow version: &#34;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">Tensorflow version:  2.2.0
</code></pre></div><p>As an example, consider computation of the quantity
\[
\left(\sin x \right)^2 + \left( \cos y \right)^2
\]
with \(x\) and \(y\) being two-dimensional arrays (matrices).
We define two functions (one without and another with the decorator) and
create the arrays:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">f_slow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="nd">@tf.function</span>
<span class="k">def</span> <span class="nf">f_fast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">4000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">4000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</code></pre></div><p>Now let&rsquo;s measure the difference in the performance:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">timeit</span>

<span class="n">t_slow</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;f_slow(x, y)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import f_slow, x, y&#39;</span><span class="p">,</span>
    <span class="n">number</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t_slow</span><span class="p">)</span>

<span class="n">t_fast</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;f_fast(x, y)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import f_fast, x, y&#39;</span><span class="p">,</span>
    <span class="n">number</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t_fast</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">1.02903594000054
0.6089697299994441
</code></pre></div><p>You can see that the runtime of the decorated function is only 60% of that
of the pure Python function.
In general, <code>tf.function</code> decorator should be used whenever you have some
computation-heavy function.
Good example is one training step for a neural network in which you need to
evaluate loss function, compute its gradient and then update the neural-network
parameters.</p>
<ul class="pa0">
  
   <li class="list">
     <a href="/tags/tensorflow" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">tensorflow</a>
   </li>
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://dmitrykabanov.com" >
    &copy; 2021 Website of Dmitry Kabanov
  </a>
    <div>










</div>
  </div>
</footer>

    

<script src="/dist/js/app.3fc0f988d21662902933.js"></script>
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
 MathJax.Hub.Config({
     tex2jax: {
         inlineMath: [['$','$'], ['\\(','\\)']],
         displayMath: [['$$','$$'], ['\\[', '\\]']],
         processEscapes: true,
         processEnvironments: true,
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
         TeX: { equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"] }
     }
 });
 MathJax.Hub.Queue(function() {
     
     
     
     var all = MathJax.Hub.getAllJax(), i;
     for(i = 0; i < all.length; i += 1) {
         all[i].SourceElement().parentNode.className += ' has-jax';
     }
 });

 MathJax.Hub.Config({
     
     TeX: {
         equationNumbers: { autoNumber: "AMS" },
         Macros: {
             C: "{\\mathrm{C}}",
             E: "{\\mathbb{E}}",
             P: "{\\mathbb{P}}",
             R: "{\\mathbb{R}}",
             normal: "{\\mathrm{N}}",
             uniform: "{\\mathrm{Unif}}",
         },
     },
 });
</script>


  </body>
</html>
